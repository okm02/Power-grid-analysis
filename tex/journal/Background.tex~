\section{Background}
\label{background}

\subsection{Resilience analysis using Graph Theory}
In scale-free networks (SF) the probability of a node being connected to $k$ others exhibits a power-law distribution $P(k) \propto k^{-\alpha}$, which is a topological property affecting and controlling their resilience or the measure of their functionality subject to disruptions \cite{Newman:2003da,Gao:2015fg,Bashan:2013cja,Gao:2015fga,2000Natur.406..378A}. Examples of this class of SF are the Internet, power systems, and transportations networks, which are real-world networks shown to be robust when subject to random failures however display a high vulnerability when prone to cascading ones \cite{Bompard:2011cd,DuenasOsorio:2009ff,2016arXiv160904310M,Cohen:2001hf}. 

In power systems, and unlike random failures which emerge locally, blackouts are severe events  associated with cascading behavior leading to global network collapse \cite{RosasCasals:2007td,Bompard:2009ga,Brummitt:2013jj,Daqing:2014bp,Albert:2004bw,Wang:2011js,Sole:2008cv}.
Examples such as the one affecting the north-east in the US and eastern Canada in 2003 burdened their economies with 10 billions dollars of direct costs \cite{Daqing:2014bp}. Such failures can be linked to either structural dependencies, where the damage spreads via structurally dependent connections, or functional overloads, where the flow goes through alternative paths leading to overloaded nodes. Thus understanding the propagation of these failures becomes pivotal in developing and deploying protective and mitigating strategies. 

\subsection{Distribued Algorithms}



 
\subsection{Distributed Computation frameworks: MapReduce, Pregel, Apache Spark and GraphX}
The last few years have witnessed an uptake in distributed data processing research. Among the leading frameworks to exploit distributed computation on commodity hardware is the MapReduce paradigm \cite{mapreduce}. 8. Dean, J., Ghemawat, S.: Mapreduce: simplified data processing on large clusters.
Commun. ACM 51(1), 107–113 (2008) 
A typical MapReduce program consists of the ``Map'' operator that parcels out work to various nodes within the cluster or map, and the ``Reduce'' phase that applies a reduction operator on the results from each node into a global query. The key contributions of the MapReduce framework are the scalability and fault-tolerance achieved for a variety of applications by optimizing the execution engine, for example, by reassigning tasks when a given execution fails. As with all other parallel and distributed computing paradigms, the performance of an efficient MapReduce algorithm is contingent upon a reduced communication cost. Of particular challenge is how to efficiently process large graphs. Graph algorithms often exhibit poor locality of reference, and a low compute-to- memory access ratio, which affects the scalability of parallel graph algorithms. It is also difficult to maintain a steady degree of parallelism over the course of execution of graph algorithms. Additionally, expressing  a graph algorithm in MapReduce requires passing the entire state of the graph from one stage to the next — imposing significant communication as well as serialisation in the parallel code. As such, the first serious development for supporting graph processing using the MapReduce framework is found in Google's Pregel \cite{}, [18]. 18. Malewicz, G., et al.: Pregel: a system for large-scale graph processing. In: SIGMOD, pp. 135–146. ACM (2010) In addition, the need to coordinate the steps of a chained MapReduce adds programming complexity that is avoided by Pregel’s iteration over supersteps. To address this, Pregel is built around the Bulk Synchronous Parallel model \cite{Biss04, McColl1, McColl2, Valiant}. 
\bibitem{Biss04}
R. H. Bisseling. {\it Parallel Scientific Computation: A structured
Approach using BSP and MPI}, Oxford University Press, NY, 2004.
\bibitem{McColl1}
J. M. D. Hill, W. F. McColl, and D. B. Skillicorn. Questions and
Answers about BSP. Report PRG-TR-15-96. Oxford University
Computing Laboratory, 1996.
\bibitem{McColl2}

J. M. D. Hill, W. F. McColl, D. C. Stefanescu, M. W. Goudrea,
K. Lang, S. B. Rao, T. Suel, T. Tsantilas, R. H. Bisseling. BSPlib:
The BSP Programming Library. {\it Parallel Computing}, 24: 1947--1980, 1998.
\bibitem{Valiant}

L. G. Valiant. A Bridging Model for Parallel Computation. {\it
Communications of the ACM}, 33: 103--111, 1990.


In a BSP algorithm, a computation proceeds in a series of global supersteps. Each superstep consists of three phases: (1) Concurrent computation, where each processor performs local computations using values stored in the local fast memory of the processor. (2)  Communication: The processes exchange data between themselves if needed. (3) Barrier synchronisation: each processor arriving at this point waits until all other processes have reached the same barrier.
Google’s scalable and fault-tolerant platform with an API that is sufficiently flexible to express arbitrary graph algorithms. Pregel keeps vertices and edges on the machine that performs computation, and uses network transfers only for messages. 

According to this model, the computation is organized as a sequence of iterations, and can be described from the point of view of a vertex, that manages its state and sends messages only to its neighbours. These frameworks [7,13] can be
characterized by four pillars: timing, communication, execution model and partitioning.




Spark, a distributed computation framework built around the MapReduce paradigm, is a recent Apache foundation software project supported by an execution engine for big data processing. Spark provides for in-memory computation, which refers to the storage of information in the main random access memory (RAM) of dedicated servers rather than in relational databases running on relatively slower disk drives. Using over 80 high-level operators, Spark makes it possible to write code more succinctly, and till this time, is considered one of the fastest frameworks for big data processing. Spark's most notable properties are also thanks to its core, which, in addition for serving as the base engine for large-scale parallel and distributed data processing, is able to handle memory management and fault recovery, scheduling, distributing and monitoring jobs on a cluster, as well as interacting with storage systems.

Spark hinges on parallel abstract data collections called RDDs (resilient distributed datasets), which can distributed across a cluster. These RDDs are immutable, partitioned data structures that can be manipulated through multiple operators like Map, Reduce, and Join. For example, RDDs are created through parallel transformations (e.g., map, group by, filter, join, create from file systems). RDDs can be cached (in-memory) by allowing to keep data sets of interest locally across operations, thus contributing to a substantial speedup. At the same time, Spark uses lineage to support fault tolerance, i.e., record all the operations/transformations that yielded RDDs from a source data. In case of failure, an RDD can be reconstructed given the transformation functions contributing to that RDD. Additionally, after creating RDDs, it is possible to analyse them using actions such as count, reduce, collect and save. Note that all operations/transformations are lazy until one runs an action. At that point, the Spark execution engine pipelines operations and determines an execution plan.

Borrowing from Pregel, GraphX~\cite{graphx} is a platform built on top of Spark that provides APIs for parallel and distributed processing on large graphs. In GraphX, each graph is mapped into different RDDs, where in each RDD one applies the computation on the graph using the “think like a vertex” model. 

