\section{Background}
\label{background}

\subsection{Resilience analysis using Graph Theory}
\label{resilience}

The seminal works in \cite{2000Natur.406..378A,DaqingAl14} address contingency analysis for the power grid using simulation of complex networks. Typically, the power grid is modeled using generator, transmission, and distribution nodes. In such scale-free networks, a large fraction of the nodes have low degrees and a substantially smaller fraction exhibit high degrees. Those networks are established to be resilient against random failures upon nodes, and the overall loss to be contingent upon the targeting of the high-degree hubs \cite{AlbertAl00, CohenAl00, CohenAl01, CallawayAl00}. 
\edited{
In addition to topological analysis of the power grid, other metrics can be used to assess its vulnerability. These are based on the physical properties of the network such as line resistance and sensitivity, which encapsulate the impedance matrix and consequently, the  ``electrical centrality'',  the equivalent of topological centrality, is computed and node removal strategies can be tested accordingly~\cite{Hines:2007bt,WangST10}. 

Equivalently, vertex vulnerability or network robustness can be evaluated through percolation measures such as the average inverse geodesic and the giant component, which emerges subsequent to attacks and exhibits a dependence on the node removal strategy and on the topology of the network under scrutiny~\cite{Bianconi:2016ka,Callaway:vd,Karrer:2014ep,Radicchi:2015gp}. 
We will thus focus our effort on a graph-based approach to vulnerability since the physical properties of the power grid were not made available to us.}

In our Spark implementation we aim to distribute the random and cascading failure scenarios explored by \cite{2000Natur.406..378A}, which in turn models the North American power grid using its transmission lines, and examines its connectivity in relation to a small set of high impact nodes. 

The notion of connectivity is based on the notion of betweenness centrality. A node's betweenness centrality is a focal measure of connectedness in a graph, built around the notion of shortest paths. Given any two vertices in a graph, there exists at least one shortest path between the vertices. When the length of the path is infinity, it is understood to say that there is no path to connect the given two vertices. If the graph is weighted, the shortest path is obtained by minimising the number of edges that the path passes through. Else, the path is obtained by minimising the sum of the weights of the edges that the path passes through. Given an arbitrary vertex in a graph, its betweenness centrality is defined to be the number of shortest paths that pass through the given vertex. Exploiting betweenness centrality has been widespread in a number of works addressing power grid vulnerability analysis. The work in \cite{2000Natur.406..378A} employs this notion using four scenarios each of which simulates a unique temporal mode of removal of vertices. For example, the overall connectivity of the graph is re-examined upon removal of transmission nodes according to the following orders: (1) totally random order (2) decreasing order of node degrees (load) (3) decreasing order of node betweennees centrality and (4) decreasing order of node betweenness centrality in cascading order resulting from the nodes' dynamical removal. \edited{
The cascading scenario is based on recalculated information or more precisely after the identification of the most central node of the initial graph and subsequent to its removal a new graph ensues. The central node of the latter should then be computed and then the process is iterated over.}

The approach taken in \cite{JinAl10} considers the intensity of the power flowing on a transmission branch, as opposed to the degree of nodes. The resulting graph is weighted (but still undirected), and the contingency analysis method there is based on applying edge betweenness centrality \cite{GirvanAl02} to the power grid topology. High-impact components in the power grid are defined using the most traversed edges. We believe this approach is less exhaustive than the one adopted by \cite{2000Natur.406..378A}, and we do not pursue it here.

Our spatial understanding of the propagation of faults in the Lebanese power grid makes classical use of the concept of spatial correlation \cite{CavagnaAl10, MakseAl95}. As a leading example that we follow, this measure is used in \cite{DaqingAl14} to measure the relation between failures separated at some distance $r$. More details follow in Sec. \ref{methods}.


\subsection{Distributed Computation frameworks}
\label{distcomp}
In the following, we discuss two distributed computation frameworks: (1) MapReduce and Pregel; (2) Spark and GraphX.

\subsubsection{MapReduce and Pregel}
The last few years have witnessed an uptake in distributed data processing research. Among the leading frameworks to exploit distributed computation on commodity hardware is the MapReduce paradigm \cite{mapreduce}. A typical MapReduce program consists of the ``Map'' operator that parcels out work to various nodes within the cluster or map, and the ``Reduce'' phase that applies a reduction operator on the results from each node into a global query. The key contributions of the MapReduce framework are the scalability and fault-tolerance achieved for a variety of applications by optimizing the execution engine, for example, by reassigning tasks when a given execution fails. As with all other parallel and distributed paradigms, the performance of an efficient MapReduce algorithm is contingent upon a reduced communication cost. Of particular challenge is how to efficiently process large graphs. Graph algorithms often exhibit poor locality of reference, and a low compute-to-memory access ratio, which affects the scalability of their parallel adaptations. It is also difficult to maintain a steady degree of parallelism over the course of execution of graph algorithms. Additionally, expressing a graph algorithm in MapReduce requires passing the entire state of the graph from one stage to the next, thus imposing significant communication as well as serialisation in the parallel code. 

The first serious development for supporting graph algorithms using the MapReduce framework is found in Google's Pregel \cite{Pregel}. Instead of coordinating the steps of a chained MapReduce program, Pregel is able to process iteration over supersteps under the Bulk Synchronous Parallel model \cite{Biss04, McColl2, Valiant}. In a BSP algorithm, a computation proceeds in a series of global supersteps. Each superstep consists of three phases:
\begin{enumerate}
\item{A concurrent computation superstep: each processor performs local computations using values stored in the local, fast memory of the processor.}
\item{A communication superstep: the processes exchange data between themselves if needed for the aggregation of the results computed in (1) above.}
\item{A barrier synchronisation superstep: each processor halts until all other processes have reached the same barrier.}
\end{enumerate}
According to this model, a graph algorithm in Pregel is organised as a sequence of iterations, and can be described from the point of view of a vertex, that manages its state and sends messages only to its neighbours. Pregel keeps vertices and edges on the machine that performs computation, and uses network transfers only for messages.

\subsubsection{Spark and GraphX}

Spark, a distributed computation framework built around the MapReduce paradigm, is a recent Apache foundation software project supported by an execution engine for big data processing. Spark provides for in-memory computation, which refers to the storage of information in the main random access memory (RAM) of dedicated servers rather than in relational databases running on relatively slower disk drives. Using over $80$ high-level operators, Spark makes it possible to write code more succinctly, and till this point in time, is considered one of the fastest frameworks for big data processing. Spark's most notable properties are also thanks to its core, which, in addition for serving as the base engine for large-scale parallel and distributed data processing, is able to handle memory management and fault recovery, scheduling, distributing and monitoring jobs on a cluster, as well as interacting with storage systems.

Spark hinges on parallel abstract data collections called RDDs (resilient distributed datasets), which can be distributed across a cluster. These RDDs are immutable, partitioned data structures that can be manipulated through multiple operators like Map, Reduce, and Join. For example, RDDs are created through parallel transformations (e.g., map, group by, filter, join, create from file systems). RDDs can be cached (in-memory) by allowing to keep data sets of interest locally across operations, thus contributing to a substantial speedup. At the same time, Spark uses lineage to support fault tolerance, i.e., record all the operations/transformations that yield RDDs from a source data. In case of failure, an RDD can be reconstructed given the transformation functions contributing to that RDD. Additionally, after creating RDDs, it is possible to analyse them using actions such as count, reduce, collect and save. Note that all operations/transformations are lazy until one runs an action. At that point, the Spark execution engine pipelines operations and determines an execution plan.

Borrowing from Pregel, GraphX~\cite{graphx} is a platform built on top of Spark that provides APIs for parallel and distributed processing on large graphs. In GraphX, each graph is mapped into different RDDs, where in each RDD one applies the computation on the graph using the ``think like a vertex'' model. 

